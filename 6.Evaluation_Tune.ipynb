{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Evaluate a single Answer\n",
    "* Context Relevance: retrieved context should provide information for the question\n",
    "* Answer Faithfulness: answer should be consist with retrieved context\n",
    "* Answer Relevance: answer should address the question\n",
    "\n",
    "## Evaluate the RAG system\n",
    "* Noise Robustness: some document is related by lack substantive information\n",
    "* Negative Rejection: don't to respond out-scope knowledge\n",
    "* Information Integration: ability to  answer based on multiple document\n",
    "* Counterfactual Robustness: distinguish possible misinformation in the document\n",
    "\n",
    "## Key Takeaways\n",
    "The Evaluation is depends on LLM, but sometimes judgement from LLM is not so reliable. We still have long journey to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureresource import (\n",
    "    get_llm,\n",
    "    get_embed_model,\n",
    "    get_vector_store\n",
    ")\n",
    "from index import get_index\n",
    "\n",
    "index_dict = {}\n",
    "llm = get_llm(\"gpt-35-turbo\", \"gpt-35-turbo-1106\")\n",
    "embed_model = get_embed_model(\"text-embedding-ada-002\", \"text-embedding-ada-002\")\n",
    "vector_store = get_vector_store(\"chunk-512\")\n",
    "index = get_index(vector_store, llm, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=False)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"在什么地点可以勾起对堂吉柯德的联想\"\n",
    "response = query_engine.query(question)\n",
    "faith_eval_result = evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import AnswerRelevancyEvaluator\n",
    "relevance_evaluator = AnswerRelevancyEvaluator(llm=llm)\n",
    "relevance_eval_result = relevance_evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import AnswerRelevancyEvaluator\n",
    "relevance_evaluator = AnswerRelevancyEvaluator(llm=llm)\n",
    "relevance_eval_result = relevance_evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import ContextRelevancyEvaluator\n",
    "context_relevance_evaluator = ContextRelevancyEvaluator(llm=llm)\n",
    "context_relevance_result = context_relevance_evaluator.evaluate(query=question, response=response, contexts=[node.get_content() for node in response.source_nodes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Question\n",
       "在哪里可以勾起对堂吉柯德的联想\n",
       "\n",
       "## Response\n",
       "在纪念碑前面的场景可以勾起对堂吉柯德的联想。\n",
       "\n",
       "## Retrieved Context\n",
       "### node_id: 41162f37-97af-4d62-a686-92477e7c74ab\n",
       "\n",
       "score: 0.84682554\n",
       "\n",
       "#### text:\n",
       "从纪念碑前面走去，你会感觉那栋稳稳当当、三台阶收分的大楼，就是纪念碑设计中的一个背景。它们作为建筑群，活像是一个整体。\n",
       "\n",
       "![image](./Images/113.jpeg)\n",
       "\n",
       "塞万提斯纪念碑\n",
       "\n",
       "蓝天和碑前面的水池，打破了“纪念”的沉闷。纪念碑的主角高高在上，却和整个纪念碑的色调没有区分。塞万提斯在那里，可是他已经和西班牙的巨石融为一体了。那石砌的纪念碑，就如同西班牙那绵绵不尽的群山。而接近地面、无可阻挡地在走出来的，是那几近黑色的两个青铜塑像，那就是骑在瘦马上的堂·吉诃德和骑在驴子上的桑丘。\n",
       "\n",
       "站在这两个一高瘦一矮胖、万世不坠的西班牙人面前，我终于感到有必要想想，假如堂·吉诃德是一个真正意义上的英雄或者骑士，假如他代表了那么多的精神和思想，他还有什么意思？他们从西班牙的黄金时代走出来，却踩着贫瘠的土地。\n",
       "### node_id: e9b14226-e3ce-4182-bec2-9c59c40ec502\n",
       "\n",
       "score: 0.84351003\n",
       "\n",
       "#### text:\n",
       "这个骑着毛驴的桑丘，是塞万提斯眼中真正的西班牙芸芸大众。桑丘并非没有英雄幻想，只是短缺堂·吉诃德式的英雄气概，且也不乏一点隐隐的私心，这才忠心耿耿、天涯海角地在瘦马后面紧紧跟随。\n",
       "\n",
       "塞万提斯向我们指点了我们每个人的英雄情结，我们是桑丘，也是堂·吉诃德。我们有时候是桑丘，有时候是堂·吉诃德。他们形影不离，可以是同一个人，可以是同一个民族，可以就是我们眼前的这个世界。我们的冲动和幻想却可能是错乱的，我们在幻想和错乱之中摸索着理性。我们不了解这个世界，因为我们不了解自己或者根本不愿意了解自己，我们无法控制那支配着我们内心的欲望和冲动。在每一个宣言后面，都肩并肩地站着他们，堂·吉诃德和桑丘。而塞万提斯，怀着点忧郁，目送他们前行。\n",
       "\n",
       "前面是又一个两百年，十八世纪和十九世纪。偏偏就在这新的两百年即将开始的时候，西班牙的王位被传给了法国路易王朝。\n",
       "\n",
       "## Evaluation result\n",
       "## Context Relevance\n",
       "The retrieved context is not directly relevant to the user's query about where to evoke thoughts of Don Quixote. The context provided describes a monument dedicated to Cervantes and Don Quixote, as well as some philosophical reflections on the characters, but it does not offer specific locations or methods for evoking thoughts of Don Quixote.\n",
       "\n",
       "The retrieved context cannot be used exclusively to provide a full answer to the user's query, as it does not offer specific locations or methods for evoking thoughts of Don Quixote. It only provides philosophical reflections and a description of a monument dedicated to the character.\n",
       "\n",
       "[RESULT] 2.0\n",
       "\n",
       "## Faithfulness\n",
       "YES\n",
       "\n",
       "## Answer Relevance\n",
       "1. The provided response matches the subject matter of the user's query by mentioning a specific location where one can evoke thoughts of Don Quixote.\n",
       "2. The provided response attempts to address the focus or perspective on the subject matter taken on by the user's query by suggesting a specific location where one can evoke thoughts of Don Quixote.\n",
       "[RESULT] 2\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "source_text = \"\\n\".join([\"### node_id: \" + node.node_id + \"\\n\\nscore: \" + str(node.score) + \"\\n\\n#### text:\\n\" + node.text for node in response.source_nodes])\n",
    "\n",
    "display(Markdown(f\"\"\"\\\n",
    "## Question\n",
    "{question}\n",
    "\n",
    "## Response\n",
    "{response.response}\n",
    "\n",
    "## Retrieved Context\n",
    "{source_text}\n",
    "\n",
    "## Evaluation result\n",
    "## Context Relevance\n",
    "{str(context_relevance_result.feedback)}\n",
    "\n",
    "## Faithfulness\n",
    "{str(faith_eval_result.feedback)}\n",
    "\n",
    "## Answer Relevance\n",
    "{str(relevance_eval_result.feedback)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = get_llm(\"gpt-4\", \"gpt-4\")\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"在什么地点可以勾起对堂吉柯德的联想\"\n",
    "response = query_engine.query(question)\n",
    "faith_eval_result = evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import AnswerRelevancyEvaluator\n",
    "relevance_evaluator = AnswerRelevancyEvaluator(llm=llm)\n",
    "relevance_eval_result = relevance_evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import AnswerRelevancyEvaluator\n",
    "relevance_evaluator = AnswerRelevancyEvaluator(llm=llm)\n",
    "relevance_eval_result = relevance_evaluator.evaluate_response(query=question, response=response)\n",
    "from llama_index.core.evaluation import ContextRelevancyEvaluator\n",
    "context_relevance_evaluator = ContextRelevancyEvaluator(llm=llm)\n",
    "context_relevance_result = context_relevance_evaluator.evaluate(query=question, response=response, contexts=[node.get_content() for node in response.source_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Question\n",
       "在什么地点可以勾起对堂吉柯德的联想\n",
       "\n",
       "## Response\n",
       "At the location described in the context, one can be reminded of Don Quixote when standing in front of the bronze statues of Don Quixote and Sancho Panza, which are close to the ground and nearly black in color.\n",
       "\n",
       "## Retrieved Context\n",
       "### node_id: 41162f37-97af-4d62-a686-92477e7c74ab\n",
       "\n",
       "score: 0.8584269\n",
       "\n",
       "#### text:\n",
       "从纪念碑前面走去，你会感觉那栋稳稳当当、三台阶收分的大楼，就是纪念碑设计中的一个背景。它们作为建筑群，活像是一个整体。\n",
       "\n",
       "![image](./Images/113.jpeg)\n",
       "\n",
       "塞万提斯纪念碑\n",
       "\n",
       "蓝天和碑前面的水池，打破了“纪念”的沉闷。纪念碑的主角高高在上，却和整个纪念碑的色调没有区分。塞万提斯在那里，可是他已经和西班牙的巨石融为一体了。那石砌的纪念碑，就如同西班牙那绵绵不尽的群山。而接近地面、无可阻挡地在走出来的，是那几近黑色的两个青铜塑像，那就是骑在瘦马上的堂·吉诃德和骑在驴子上的桑丘。\n",
       "\n",
       "站在这两个一高瘦一矮胖、万世不坠的西班牙人面前，我终于感到有必要想想，假如堂·吉诃德是一个真正意义上的英雄或者骑士，假如他代表了那么多的精神和思想，他还有什么意思？他们从西班牙的黄金时代走出来，却踩着贫瘠的土地。\n",
       "### node_id: 7180f127-0f57-49ce-b846-1bbbaa48ae8e\n",
       "\n",
       "score: 0.85400695\n",
       "\n",
       "#### text:\n",
       "已经不记得我们都点了什么饮料，却能记得坐在阴影里，看着夕阳下的马约尔广场：一张张小桌子边是懒懒散散的游客，远处是画家们的摊位，还有靠歌唱谋生的艺术家，不失时机地弹起吉他唱起来——那是一种微微有点奇特的感觉，即便在今天，我们回想起来，仍然能够体会到广场那一丝由规整而起的内在拘谨。围绕广场四周的建筑，不论是红墙还是白石，都由“时间”调入了一种只属于历史的黄色。红白之间就不仅只有设计师造就的色彩对比，还有岁月引出的色彩调和。感觉的奇特，源自于这里被历史做旧了的建筑形制和色彩氛围。置身其中，犹如不留神一脚踏进了历史。\n",
       "\n",
       "马德里作为首都，当然也见证过宗教裁判。这里就曾经是一个公审公判和行刑的地方。现在，西班牙还保留了艺术家在1683年所画的这个广场在1680年6月30日审判新教异端的场景。审判的时候，连国王都来了，除了四周楼房的窗口，还在广场两侧搭起一层层的看台。那些不肯悔过的新教徒，会在当晚被处死。\n",
       "\n",
       "## Evaluation result\n",
       "## Context Relevance\n",
       "The retrieved context does not match the subject matter of the user's query. The context talks about the Cervantes Monument in Madrid, Spain, and the surrounding area, but it does not specifically address the location where one can evoke thoughts of Don Quixote. Therefore, the relevance of the retrieved context to the user's query is low.\n",
       "\n",
       "The retrieved context cannot be used exclusively to provide a full answer to the user's query. It does not provide specific information about the location where one can evoke thoughts of Don Quixote, which is what the user is asking for. Therefore, the context is not sufficient to fully answer the user's query.\n",
       "\n",
       "[RESULT] 2.0\n",
       "\n",
       "## Faithfulness\n",
       "YES\n",
       "\n",
       "## Answer Relevance\n",
       "1. The provided response matches the subject matter of the user's query as it mentions a specific location that can evoke thoughts of Don Quixote.\n",
       "2. The provided response attempts to address the focus or perspective on the subject matter taken on by the user's query by describing a specific location and the visual cues that can evoke thoughts of Don Quixote.\n",
       "\n",
       "[RESULT] 2\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "source_text = \"\\n\".join([\"### node_id: \" + node.node_id + \"\\n\\nscore: \" + str(node.score) + \"\\n\\n#### text:\\n\" + node.text for node in response.source_nodes])\n",
    "\n",
    "display(Markdown(f\"\"\"\\\n",
    "## Question\n",
    "{question}\n",
    "\n",
    "## Response\n",
    "{response.response}\n",
    "\n",
    "## Retrieved Context\n",
    "{source_text}\n",
    "\n",
    "## Evaluation result\n",
    "## Context Relevance\n",
    "{str(context_relevance_result.feedback)}\n",
    "\n",
    "## Faithfulness\n",
    "{str(faith_eval_result.feedback)}\n",
    "\n",
    "## Answer Relevance\n",
    "{str(relevance_eval_result.feedback)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     VectorStoreIndex,\n\u001b[1;32m      3\u001b[0m     load_index_from_storage,\n\u001b[1;32m      4\u001b[0m     StorageContext,\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam_tuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamTuner\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam_tuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TunedResult, RunResult\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     get_responses,\n\u001b[1;32m     10\u001b[0m     aget_responses,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.experimental'"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.experimental.param_tuner import ParamTuner\n",
    "from llama_index.core.param_tuner.base import TunedResult, RunResult\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    "    aget_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    SemanticSimilarityEvaluator,\n",
    "    BatchEvalRunner,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_index(chunk_size, docs):\n",
    "    index_out_path = f\"./storage_{chunk_size}\"\n",
    "    if not os.path.exists(index_out_path):\n",
    "        Path(index_out_path).mkdir(parents=True, exist_ok=True)\n",
    "        # parse docs\n",
    "        node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
    "        base_nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "        # build index\n",
    "        index = VectorStoreIndex(base_nodes)\n",
    "        # save index to disk\n",
    "        index.storage_context.persist(index_out_path)\n",
    "    else:\n",
    "        # rebuild storage context\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            persist_dir=index_out_path\n",
    "        )\n",
    "        # load index\n",
    "        index = load_index_from_storage(\n",
    "            storage_context,\n",
    "        )\n",
    "    return index\n",
    "\n",
    "\n",
    "def _get_eval_batch_runner():\n",
    "    evaluator_s = SemanticSimilarityEvaluator(embed_model=OpenAIEmbedding())\n",
    "    eval_batch_runner = BatchEvalRunner(\n",
    "        {\"semantic_similarity\": evaluator_s}, workers=2, show_progress=True\n",
    "    )\n",
    "\n",
    "    return eval_batch_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params_dict):\n",
    "    chunk_size = params_dict[\"chunk_size\"]\n",
    "    docs = params_dict[\"docs\"]\n",
    "    top_k = params_dict[\"top_k\"]\n",
    "    eval_qs = params_dict[\"eval_qs\"]\n",
    "    # ref_response_strs = params_dict[\"ref_response_strs\"]\n",
    "\n",
    "    # build index\n",
    "    index = _build_index(chunk_size, docs)\n",
    "\n",
    "    # query engine\n",
    "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
    "\n",
    "    # get predicted responses\n",
    "    pred_response_objs = get_responses(\n",
    "        eval_qs, query_engine, show_progress=True\n",
    "    )\n",
    "\n",
    "    # run evaluator\n",
    "    # NOTE: can uncomment other evaluators\n",
    "    eval_batch_runner = _get_eval_batch_runner()\n",
    "    eval_results = eval_batch_runner.evaluate_responses(\n",
    "        eval_qs, responses=pred_response_objs\n",
    "    )\n",
    "\n",
    "    # get semantic similarity metric\n",
    "    mean_score = np.array(\n",
    "        [r.score for r in eval_results[\"semantic_similarity\"]]\n",
    "    ).mean()\n",
    "\n",
    "    return RunResult(score=mean_score, params=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.epub import EpubReader\n",
    "from llama_index.experimental.param_tuner import ParamTuner\n",
    "\n",
    "document = EpubReader().load_data(\"data/book.epub\")\n",
    "from testcase import question_list, bcolors\n",
    "\n",
    "param_dict = {\"chunk_size\": [256, 512, 1024], \"top_k\": [2, 5]}\n",
    "# param_dict = {\n",
    "#     \"chunk_size\": [256],\n",
    "#     \"top_k\": [1]\n",
    "# }\n",
    "fixed_param_dict = {\n",
    "    \"docs\": document,\n",
    "    \"eval_qs\": question_list,\n",
    "    # \"ref_response_strs\": ,\n",
    "}\n",
    "\n",
    "param_tuner = ParamTuner(\n",
    "    param_fn=objective_function,\n",
    "    param_dict=param_dict,\n",
    "    fixed_param_dict=fixed_param_dict,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = param_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.best_run_result\n",
    "best_top_k = results.best_run_result.params[\"top_k\"]\n",
    "best_chunk_size = results.best_run_result.params[\"chunk_size\"]\n",
    "print(f\"Score: {best_result.score}\")\n",
    "print(f\"Top-k: {best_top_k}\")\n",
    "print(f\"Chunk size: {best_chunk_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
