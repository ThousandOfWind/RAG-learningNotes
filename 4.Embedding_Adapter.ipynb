{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding\n",
    "\n",
    "## Choose a model which is special for your scenarios\n",
    "There are many different embedding model in the market, each have their own advantage\n",
    "\n",
    "https://openai.com/blog/introducing-text-and-code-embeddings\n",
    "|Models\t|Use |Cases|\n",
    "|--|--|--|\n",
    "|Text similarity: Captures semantic similarity between pieces of text.|\ttext-similarity-{ada, babbage, curie, davinci}-001\t|Clustering, regression, anomaly detection, visualization|\n",
    "|Text search: Semantic information retrieval over documents.\t|text-search-{ada, babbage, curie, davinci}-{query, doc}-001\t|Search, context relevance, information retrieval|\n",
    "|Code search: Find relevant code with a query in natural language.\t|code-search-{ada, babbage}-{code, text}-001\t|Code search and relevance|\n",
    "\n",
    "## Mix/hybrid Retrieval\n",
    "Use both Dense model and sparse model by leverage complementary. Dense-Sparse is to describe the feature\n",
    "* Dense model: good at dense feature -- age, score\n",
    "* Sparse model: good at sparse feature -- sentence, keywords\n",
    "\n",
    "\n",
    "## Finetune Embedding\n",
    "* possible to enhance domain knowledge in the embedding. It will be help if your scenarios is target to a special task with many unique terminologies, instead of people common lives\n",
    "* align retriever and generator, usually use reinforcement which use reward signal to finetune.\n",
    "\n",
    "Here is a example, https://docs.llamaindex.ai/en/stable/use_cases/fine_tuning/?h=adapter#finetuning-embeddings\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adapter\n",
    "\n",
    "Fineture Embedding model is hard because embedding model is just too big, fineture such model cost too much, and hard to guarantee the finally model.\n",
    "\n",
    "Adapter method introduce a lighte-wight model on top of exsiting embedding model\n",
    "\n",
    "* question embedding -> [Adapter] -> task classcification -> [Task Processor]\n",
    "* question embedding -> [Adapter] -> new embedding -> [Retriever]\n",
    "\n",
    "Example: https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding_adapter/"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}